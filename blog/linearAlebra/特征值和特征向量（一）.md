# 特征值和特征向量（一）

> 通过计算一个矩阵的n次方，引出解释了特征值和特征向量，介绍了特征向量特征值的计算方法，并且解释了矩阵A的n次方的神奇现象，最后抛出了一个坏消息，一个好消息

## 介绍特征值

此部分是线性代数中新的部分，所有的部分都根据 $Ax=λx$ ，所有的矩阵都为方阵。

### 引入**例子**：计算矩阵 $ A^n $

$$ A=\begin{bmatrix} 0.8 & 0.3 \\ 0.2 & 0.7 \end{bmatrix} \tag{1}$$ 

计算 $A$ $A^2$ $A^3$ $...$ $A^{100}$ ，会惊人的发现指数位从14开始往后，答案就无限接近于

$$ A^{14} = A^{15} = ... = A^{100}=\begin{bmatrix} 0.6 & 0.6 \\ 0.4 & 0.4 \end{bmatrix} \tag{2}$$

其中蕴含了怎样的客观规律呢？特征值和特征向量是窥探矩阵奥秘的一个新的角度

### 什么是**特征值**？

在解释什么是特征值之前，先了解什么是特征向量。向量 $x$ 经过线性变换（被矩阵$A$ 乘）往往会改变方向，但是如果没有改变方向，即，从核心式子说起：

$$ Ax = λx  $$ ，那么该向量就是矩阵A的特征向量，对应的$λ$为特征值，特征值$λ$ 反应了特征向量经过线性变换后的情况，是伸长了还是缩短了还是反向了。

#### 计算特征向量 $x$ 特征值$λ$

1. 计算$det(A-λI) = 0 $ 得到特征值$λ$ 
2. 将计算得到的$λ$带入核心式子 $Ax = λx $，计算特征值对应的特征向量

> λ的个数有几个呢？这个要问行列式了

举例来说$$ A=\begin{bmatrix} 0.8 & 0.3 \\ 0.2 & 0.7 \end{bmatrix} \tag{3}$$ 

$$det(A-λI) = det\begin{bmatrix} 0.8 - λ & 0.3 \\ 0.2 & 0.7-λ \end{bmatrix}$$

解得 $λ_1 = 1 , λ_2 = 1/2$ ，即 $ Ax_1 = x_1, Ax_2 = 0.5x $ ,$x_1 = \begin{bmatrix}0.6 \\ 0.4\end{bmatrix}$ $x_2 = \begin{bmatrix}1 \\ -1\end{bmatrix}$

#### 解释$A^N$

$A^2x_1 = A * Ax_1 = Ax_1$

$A^2x_2 = A * Ax_2  = 0.5 * Ax_2 = 0.25x_2$

其他的向量可以由这两个向量线性组成，发现$\begin{bmatrix}0.8 \\ 0.2\end{bmatrix} = x_1+0.2x_2$ ,同样的 $\begin{bmatrix}0.3 \\ 0.7\end{bmatrix} = x_1 - 0.3x_2$

所以$A = \begin{bmatrix}x_1+0.2x_2 , x_1-0.3x_2\end{bmatrix}$ 

$A^{99}\begin{bmatrix}0.8\\0.2\end{bmatrix} =A^{99}*(x_1+0.2x_2)=A^{99}x_1+0.2A^{99}x_2=x_1+0.2*(1/2)^{99}x_2 $

$A^{99}\begin{bmatrix}0.3\\0.7\end{bmatrix} =A^{99}*(x_1-0.3x_2)=A^{99}x_1-0.3A^{99}x_2=x_1-0.3(1/2)^{99}x_2 $

所以 $A^{100} = A^{99} * A =  [x_1, x_1] = \begin{bmatrix}0.6 \ 0.6\\0.4 \ 0.4  \end{bmatrix}$

#### 一个好消息一个坏消息

- 好消息：特征值的乘积为行列式的值、和为矩阵对角元素的和，
- 坏消息：特征值随着矩阵